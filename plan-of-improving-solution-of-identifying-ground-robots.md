# 📊 HNURM vs PFA 纯相机识别方案对比分析

### 一、透视变换 —— 最关键的差距

| 维度         | PFA（参考）                                | HNURM（现状）                       |
| ------------ | ------------------------------------------ | ----------------------------------- |
| **高度分层** | ✅ 地面层 + 高地层两套变换矩阵              | ❌ **仅一套 Homography，不区分高度** |
| **分层判定** | 用掩码图 `mask_image` 自动判定目标在哪一层 | 无                                  |
| **标定工具** | PyQt5 GUI，在图像和地图上双向点击          | 终端手输坐标 + cv2 窗口选点         |

**🔴 这是最大短板。** RoboMaster 赛场有中央高地、环形高地等地形。HNURM 的单层 Homography 假设所有机器人都在同一平面上——**当机器人站在高地上时，透视变换会产生巨大误差（可达数米）**。

> **建议**：
> 1. 仿照 PFA，为**地面层和高地层**分别标定两套透视变换矩阵
> 2. 制作**分区掩码图**（make_mask.py 类似），映射后先查掩码判定目标属于哪层，再选用对应矩阵
> 3. 标定流程也可以同时在**地图图像**上点选对应点，更直观

---

### 二、坐标滤波 —— 缺少时序平滑

| 维度           | PFA                                                          | HNURM                                  |
| -------------- | ------------------------------------------------------------ | -------------------------------------- |
| **坐标滤波**   | ✅ 卡尔曼滤波（`EnhancedKalmanFilter`，动态噪声）/ 滑动窗口滤波二选一 | ❌ **直接发布原始变换坐标，无任何滤波** |
| **异常值剔除** | ✅ 大跳变自动丢弃                                             | ❌ 无                                   |
| **超时清理**   | ✅ 2 秒未更新自动移除                                         | 仅有 `life_time` 帧数倒计              |

**🔴 无滤波的后果**：小地图上坐标会**逐帧跳动/抖动**，尤其在远距离或低分辨率情况下，变换噪声很大。

> **建议**：
> 1. 为每个机器人 ID 维护一个**卡尔曼滤波器**（4 状态: $x, y, v_x, v_y$），使用 `cv2.KalmanFilter` 即可
> 2. 增加**异常值检测**：单帧跳变超过阈值时丢弃
> 3. 可参考 PFA 的 `EnhancedKalmanFilter`，根据位移大小动态调整过程噪声

---

### 三、推理性能 —— 串行瓶颈

| 维度              | PFA                                                | HNURM                                                        |
| ----------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| **推理后端**      | `DetectMultiBackend`（支持 PyTorch/TensorRT/ONNX） | Ultralytics `.pt`（仅 PyTorch）                              |
| **FP16 半精度**   | ✅ 默认开启                                         | ❌ 未使用                                                     |
| **TensorRT 加速** | ✅ 提供 `export.py` + `onnx2engine.py` 工具链       | ❌ 无                                                         |
| **二级推理**      | 每个 ROI 单独 `predict()`，但 `max_det=1` 降低开销 | 每个 ROI 调用**两次** `predict()`（Stage2 + Stage3），且无 batch |

**🔴 HNURM 的三阶段推理**（在 camera_detector.py 中）：每个检测框分别调用 Stage2 和 Stage3 两次推理，假设场上 6 台机器人，**一帧需要 1 + 6×2 = 13 次推理**。

> **建议**：
> 1. 将 `.pt` 模型导出为 **TensorRT `.engine`** 格式，推理速度可提升 3-5 倍
> 2. 开启 **FP16** 半精度推理
> 3. 将多个 ROI **batch 化**处理，减少 GPU 调度开销
> 4. 考虑合并 Stage2/Stage3 为单模型，或用一次推理同时输出彩色+灰色装甲板

---

### 四、盲区处理 —— 完全缺失

| 维度         | PFA                                                         | HNURM                |
| ------------ | ----------------------------------------------------------- | -------------------- |
| **盲区预测** | ✅ 英雄/工程/哨兵有预设盲区坐标，丢失 2 秒后自动发送预测位置 | ❌ **无任何盲区处理** |
| **预测策略** | 根据标记进度变化动态切换预测点，有时间衰减                  | 无                   |

**🔴 影响**：机器人进入视野盲区后，坐标直接消失，操作手失去目标信息。

> **建议**：
> 1. 为**关键角色**（英雄、工程、哨兵）设置盲区预测点位
> 2. 当某目标超过 N 秒未检测到时，发送**最后已知位置或预设预测点**
> 3. 可结合卡尔曼滤波的**预测步**（`predict` without `correct`）做短期轨迹外推

---

### 五、灰度阈值 —— 硬编码不鲁棒

| 维度         | PFA                          | HNURM                              |
| ------------ | ---------------------------- | ---------------------------------- |
| **灰色判定** | 装甲板直接训练了含灰色的类别 | 硬编码灰度阈值（蓝 < 35, 红 < 15） |

当前代码在 camera_detector.py 中：

```python
if label < 6 and cv2.mean(gray_region)[0] < 35:   # 蓝方硬编码
    ...
elif label > 5 and cv2.mean(gray_region)[0] < 15:  # 红方硬编码
```

**🔴 问题**：赛场光照变化（白天/晚上、不同场馆）会导致阈值失效。

> **建议**：将阈值移到 detector_config.yaml 中作为可配参数，或用自适应阈值（OTSU 等）

---

### 六、其他可优化点

| 问题                       | 详情                                                         | 建议                                                     |
| -------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- |
| **Track_value 固定 10000** | 预分配 10000 个 ID 的投票表（camera_detector.py），浪费内存且长时间运行可能溢出 | 改用 `defaultdict` 动态分配，过期 ID 定时清理            |
| **配置硬编码绝对路径**     | 所有路径写死在代码中（camera_detector.py）                   | 使用 ROS2 参数或 `ament_index_python` 获取包路径         |
| **无裁判系统数据交互**     | PFA 可接收标记进度、双倍易伤等数据做决策                     | 扩展 `judge_messager` 节点，接收裁判系统数据用于辅助决策 |
| **海康相机被注释**         | 实际比赛需手动改代码切换                                     | 用 ROS2 参数 `camera_type` 动态切换                      |

---

### 📋 优先级排序

| 优先级   | 优化项                      | 预期收益                            |
| -------- | --------------------------- | ----------------------------------- |
| 🔴 **P0** | **多层透视变换 + 掩码分区** | 消除高地定位误差（数米级 → 分米级） |
| 🔴 **P0** | **坐标卡尔曼滤波**          | 消除坐标抖动，输出更稳定            |
| 🟡 **P1** | **TensorRT / FP16 加速**    | 帧率提升 3-5 倍                     |
| 🟡 **P1** | **盲区预测**                | 提升操作手信息连续性                |
| 🟢 **P2** | ROI batch 推理              | 减少多目标时延迟                    |
| 🟢 **P2** | 灰度阈值可配 + 自适应       | 提升不同光照下鲁棒性                |
| 🟢 **P2** | 配置路径 / 相机类型参数化   | 提升工程可维护性                    |

**总结**：HNURM 的纯相机方案在**检测-分类-投票**的流程上与 PFA 接近，核心差距在于 **① 缺少多层透视变换** 和 **② 缺少坐标滤波**，这两项直接影响定位精度和稳定性，建议优先实现。

---

### ⚠️ 待办：赛季地图替换

> **当前状态**：开发阶段暂用 PFA 2025 赛季地图（已放置在 `map/` 目录下），功能验证完毕后需替换为 2026 赛季地图。

**需要替换的地图文件**（均在 `map/` 目录下）：

| 当前文件 | 用途 | 替换时机 |
| --- | --- | --- |
| `pfa_map_2025.jpg` (1500×2800) | 小地图显示底图 | 获得 2026 赛季官方地图后 |
| `pfa_map_mask_2025.jpg` (2800×1500) | 高度分区掩码（黑=地面，绿=高地） | 用 `make_mask` 工具基于 2026 地图重新绘制 |
| `pfa_map_red_2025.jpg` / `pfa_map_blue_2025.jpg` | 红蓝方视角地图（标定参考用） | 获得 2026 地图后制作对应版本 |

**替换步骤**：
1. 获取 2026 赛季标准地图 → 放入 `map/` 目录
2. 运行 `ros2 run hnurm_radar make_mask --map map/2026_map.jpg --out map/2026_mask.png` 绘制新掩码
3. 更新 `camera_detector.py` 中的 `MAP_IMAGE_PATH` 和 `MASK_IMAGE_PATH`
4. 删除旧标定文件 `configs/perspective_calib.json`，重新标定透视变换
5. 如需红蓝方视角地图，将标准地图 180° 旋转生成蓝方/红方版本

**关于红方/蓝方地图的说明**：
PFA 为红蓝双方各准备了一张 180° 旋转的地图（`map_red` 和 `map_blue` 互为 180° 翻转），目的是让操作手在标定和使用时**始终看到己方基地在下方**，更符合直觉。当前我们用单张横置地图 + 代码中坐标镜像翻转实现同等效果。如果后续需要改进标定体验，可以制作红蓝双视角地图。

## 纯相机方案标定改动

### 1. 新增 GUI 标定工具
创建了 perspective_calibrator.py，参考 PFA 方案的核心思路：

| 特性 | 旧方案（终端） | 新方案（GUI） |
|---|---|---|
| 选像素点 | 在相机图上点击，然后在终端手动输入坐标 | **左图点相机，右图点地图**，自动配对 |
| 坐标输入 | 手打 `14.0,7.5`（易出错、共线） | 直接在地图上点击，自动转换为米坐标 |
| 多层标定 | 终端 y/n 交互 | 点击「切换到高地层」按钮 |
| 撤销操作 | 无 | 「撤销上一点」按钮 |
| 验证 | 无 | 自动计算重投影误差 |
| 实时预览 | 无 | 标定前实时相机画面更新 |

### 2. 操作流程

```
ros2 run hnurm_radar perspective_calibrator
```

1. 启动后左侧显示相机画面（test 模式用静态图），右侧显示赛场地图
2. 点击 **「开始标定」** 冻结画面
3. **左图点一个特征点** → **右图点对应地图位置**，交替操作至少 4 对
4. （可选）点击 **「切换到高地层」** 标定高地 4+ 对点
5. 点击 **「保存计算」** → 自动计算 Homography、显示重投影误差、保存到 configs/perspective_calib.json
6. 关闭窗口后运行 `ros2 run hnurm_radar camera_detector`

### 3. camera_detector 改动

`_interactive_calibrate()` 不再阻塞 ROS2 节点做标定，改为**提示用户运行独立的 GUI 标定工具**。如果没有标定文件，节点会打印清晰的指引信息然后以无标定模式继续运行。

### 4. 关键坐标映射

在地图上点击时，像素坐标自动按以下公式转换为赛场米坐标：

$$field\_x = \frac{map\_px}{2800} \times 28$$

$$field\_y = \frac{1500 - map\_py}{1500} \times 15$$

这与 `_draw_minimap` 和 `pixel_to_field` 使用的映射完全一致。

已进行更改。